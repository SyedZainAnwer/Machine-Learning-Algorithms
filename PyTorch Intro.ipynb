{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4Sb4x-UUeMw"
      },
      "source": [
        "# Experimenting with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7fTCxrxUeMy"
      },
      "source": [
        "For using PyTorch you can either use your own Computer or [Google Colab](https://colab.research.google.com/).\n",
        "\n",
        "You need to install the [PyTorch](https://pytorch.org/) package which comes with some extra dependencies.\n",
        "\n",
        "Install the following packages for this notebook:\n",
        "- **PyTorch**\n",
        "- **torchvision**\n",
        "- **tqdm**\n",
        "- **matplotlib**\n",
        "\n",
        "If your computer is equpped with a GPU you can also install the GPU version of PyTorch. Otherwise install the CPU version, which is smaller in size and enough for the tasks of this practical.\n",
        "\n",
        "For using the GPU version you need to fullfill some prerequisites first, which are a little time consuming.\n",
        "- Make sure that your graphics card is new enough to handle the PyTorch environment. This can be checked by searching for the compute capability of your GPU and the compute capability requirements from the PyTorch module\n",
        "- Install the latest NVIDIA driver\n",
        "- Install suitable CUDA version\n",
        "- Install CudNN\n",
        "- Install PyTorch after all previous successful steps\n",
        "\n",
        "\n",
        "Using Google Colab should avoid installing the above mentioned prerequisites."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h6SoP6CUeMy"
      },
      "source": [
        "## PyTorch Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvB8nYPSUeMz",
        "outputId": "4f427ed7-d095-4700-a94b-e5561935dcca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numpy Array: [[ 0.78104657]\n",
            " [ 0.6895362 ]\n",
            " [-0.5942594 ]\n",
            " [ 0.9869146 ]\n",
            " [ 1.6282034 ]\n",
            " [ 0.9405896 ]]\n",
            "Torch Tensor Array: tensor([[ 0.7810],\n",
            "        [ 0.6895],\n",
            "        [-0.5943],\n",
            "        [ 0.9869],\n",
            "        [ 1.6282],\n",
            "        [ 0.9406]])\n",
            "Shape: torch.Size([6, 1])\n",
            "Type: torch.float32\n",
            "Device: cpu\n",
            "Slice1: tensor([ 0.7810,  0.6895, -0.5943,  0.9869,  1.6282])\n",
            "Slice2: tensor([0.9406])\n",
            "Slice3: tensor([0.6895, 0.9869, 0.9406])\n",
            "TensorA: tensor([[ 1.2371,  1.2850, -1.2217],\n",
            "        [ 0.2899, -0.0540,  0.0033]])\n",
            "TensorB: tensor([[ 0.5321,  0.3738,  1.4336, -0.8502],\n",
            "        [ 1.1178,  1.1776, -1.0299, -1.5111],\n",
            "        [-0.2748,  0.2761,  0.1442,  0.0311]])\n",
            "Multiplication: tensor([[ 2.4303,  1.6383,  0.2739, -3.0314],\n",
            "        [ 0.0930,  0.0457,  0.4717, -0.1647]])\n",
            "TensorC: tensor([[ 0.0145,  0.4095],\n",
            "        [ 0.6612, -0.5528]])\n",
            "TensorD: tensor([[-0.2790, -0.9210],\n",
            "        [-1.4748, -1.6989]])\n",
            "Element Wise: tensor([[-0.0040, -0.3771],\n",
            "        [-0.9751,  0.9391]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Tensors\n",
        "\n",
        "# Initialize a 1d torch tensor of size (6, 1) and name it 'data'. Initialize the tensor as random normal distribution\n",
        "\n",
        "# Code here\n",
        "data = torch.randn(6,1)\n",
        "\n",
        "\n",
        "\n",
        "# Convert the torch tensor to a numpy array and convert it back afterwards. Keep the variable naming and just override the variable every time\n",
        "\n",
        "# Code here\n",
        "data = data.numpy()\n",
        "print(f\"Numpy Array: {data}\")\n",
        "data = torch.from_numpy(data)\n",
        "print(f\"Torch Tensor Array: {data}\")\n",
        "\n",
        "\n",
        "# Tensors have a shape, a data type and are executed on some device on your computer. Find the mentioned tensor attributes and print them.\n",
        "\n",
        "# Code here\n",
        "print(f\"Shape: {data.shape}\")\n",
        "print(f\"Type: {data.dtype}\")\n",
        "print(f\"Device: {data.device}\")\n",
        "\n",
        "\n",
        "# Slicing works the same as with numpy arrays. No need to learn a new syntax here :)\n",
        "# Try some slicing methods (i. e. the slicing methods we discussed in the first practical)\n",
        "\n",
        "# Code here\n",
        "slice_1 = data[0:5, -1]\n",
        "print(f\"Slice1: {slice_1}\")\n",
        "slice_2 = data[-1, :]\n",
        "print(f\"Slice2: {slice_2}\")\n",
        "slice_3 = data[1::2, 0]\n",
        "print(f\"Slice3: {slice_3}\")\n",
        "\n",
        "\n",
        "###\n",
        "# Arithmetic operations\n",
        "###\n",
        "\n",
        "# Perform a matrix multiplication with two random tensors of different shape. The value initialization is of your choice.\n",
        "\n",
        "# Code here\n",
        "tensor_a = torch.randn(2, 3)\n",
        "print(f\"TensorA: {tensor_a}\")\n",
        "\n",
        "tensor_b = torch.randn(3, 4)\n",
        "print(f\"TensorB: {tensor_b}\")\n",
        "\n",
        "result = torch.matmul(tensor_a, tensor_b)\n",
        "print(f\"Multiplication: {result}\")\n",
        "\n",
        "\n",
        "# Perform the hadamard (element-wise) product with two random initialized tensors.\n",
        "\n",
        "# Code here\n",
        "tensor_c = torch.randn(2,2)\n",
        "print(f\"TensorC: {tensor_c}\")\n",
        "\n",
        "tensor_d = torch.randn(2,2)\n",
        "print(f\"TensorD: {tensor_d}\")\n",
        "\n",
        "result = tensor_c * tensor_d\n",
        "print(f\"Element Wise: {result}\")\n",
        "\n",
        "\n",
        "\n",
        "# For more useful tensor operations, plese check out their website: https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBO3SJsVUeMz"
      },
      "source": [
        "## PyTorch Sequential and Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S92xbSwqUeMz",
        "outputId": "ab5fe926-132e-441b-e5cd-05154ca91099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer: Linear(in_features=16, out_features=32, bias=True)\n",
            "Weight of Layer: Parameter containing:\n",
            "tensor([[ 0.1347, -0.2457,  0.1130,  0.1271, -0.1047, -0.1013, -0.1368,  0.2022,\n",
            "          0.0661, -0.1726,  0.2174,  0.2026,  0.0612,  0.1608, -0.2373, -0.0650],\n",
            "        [-0.2443,  0.0154,  0.0773, -0.0662,  0.0367, -0.0072, -0.1493, -0.2162,\n",
            "          0.1317,  0.0918,  0.0048, -0.0440, -0.0442, -0.1479, -0.1198,  0.0649],\n",
            "        [ 0.0777, -0.2412,  0.0467,  0.2249, -0.0083,  0.0512, -0.0103, -0.1767,\n",
            "         -0.1162, -0.0370, -0.0784,  0.1421,  0.0448,  0.1596, -0.0639, -0.2179],\n",
            "        [-0.1569,  0.1612,  0.0834, -0.0443,  0.2000,  0.0798, -0.0607, -0.1404,\n",
            "         -0.0337, -0.1765,  0.0203, -0.1239,  0.0259, -0.0917, -0.2364,  0.1803],\n",
            "        [-0.1054, -0.2293, -0.0201,  0.0607, -0.0131,  0.2360, -0.1729, -0.0092,\n",
            "         -0.1011,  0.0769,  0.0191, -0.0464,  0.0196,  0.1908, -0.1492,  0.2465],\n",
            "        [-0.0920, -0.2194, -0.1360, -0.1563,  0.2074, -0.2065, -0.0774,  0.2077,\n",
            "          0.2332, -0.2492, -0.1657,  0.1072, -0.1076, -0.0219, -0.0134,  0.0894],\n",
            "        [-0.2057, -0.1878,  0.1715, -0.1212,  0.0184,  0.1458, -0.1320,  0.2057,\n",
            "         -0.0290, -0.0775, -0.0328, -0.0692,  0.0508,  0.0948,  0.0806,  0.0297],\n",
            "        [ 0.0909,  0.2288,  0.1713,  0.0521, -0.1880, -0.0612, -0.0041, -0.1811,\n",
            "         -0.0458,  0.0352, -0.0485,  0.0936, -0.2144, -0.1987, -0.1011, -0.1572],\n",
            "        [ 0.0689, -0.1107,  0.2063, -0.1094, -0.2168,  0.1132, -0.1133,  0.1916,\n",
            "         -0.1134, -0.0440,  0.2416,  0.1563,  0.1300, -0.2223, -0.0980, -0.0975],\n",
            "        [ 0.1585,  0.0743,  0.1369,  0.2431, -0.0079,  0.2067,  0.1004, -0.0260,\n",
            "          0.0453, -0.2038, -0.1263,  0.0565,  0.0330,  0.0376,  0.1776,  0.0562],\n",
            "        [ 0.0519,  0.0445,  0.1813, -0.1012,  0.1995,  0.1077, -0.1745, -0.1869,\n",
            "          0.2486, -0.0015,  0.1481, -0.1786, -0.0589, -0.1434, -0.2421, -0.2266],\n",
            "        [-0.1390,  0.0430,  0.2314, -0.0569, -0.0619,  0.1386,  0.0975,  0.2344,\n",
            "         -0.1535, -0.0688, -0.0109,  0.1245,  0.2447,  0.1967, -0.2178,  0.1423],\n",
            "        [ 0.0653, -0.0421,  0.0281, -0.1713,  0.2263, -0.2028, -0.0045,  0.0544,\n",
            "         -0.0551,  0.0346, -0.0579,  0.1006, -0.0203, -0.2049,  0.1609,  0.0244],\n",
            "        [ 0.1237,  0.0281,  0.1994, -0.0399,  0.0632, -0.1266, -0.0582,  0.2037,\n",
            "         -0.1378, -0.2279, -0.1022, -0.2028,  0.0892, -0.0810,  0.1418,  0.1529],\n",
            "        [-0.1837, -0.0604,  0.1513, -0.1934, -0.1662,  0.1898,  0.2344, -0.1776,\n",
            "          0.1891,  0.2360,  0.2210, -0.1032, -0.0599, -0.0031, -0.0664,  0.2066],\n",
            "        [ 0.0809,  0.0247, -0.1563,  0.0740,  0.1628, -0.0667, -0.1707, -0.1717,\n",
            "          0.0727,  0.1642,  0.2482,  0.2138, -0.0460, -0.1706, -0.2286, -0.0026],\n",
            "        [ 0.1094, -0.2397, -0.2142, -0.1492,  0.1741, -0.1763,  0.1988, -0.1626,\n",
            "         -0.0377,  0.1926,  0.1761,  0.1116, -0.1346, -0.2262,  0.0961,  0.1266],\n",
            "        [ 0.1502,  0.0854,  0.0812,  0.2269,  0.2394,  0.0899, -0.1300, -0.2178,\n",
            "          0.1759, -0.1691, -0.0122,  0.1705, -0.2074, -0.0012, -0.0849,  0.0477],\n",
            "        [ 0.2264, -0.0551,  0.2293, -0.1083,  0.2261,  0.0773, -0.0602, -0.2023,\n",
            "         -0.0923,  0.2019, -0.0552,  0.2189,  0.0552,  0.1458,  0.1489, -0.1564],\n",
            "        [ 0.1335, -0.2348, -0.1200,  0.0384,  0.1310, -0.1347, -0.1971, -0.1161,\n",
            "          0.2334, -0.0685, -0.1398, -0.0349,  0.1288, -0.1284, -0.2095,  0.2223],\n",
            "        [ 0.1533,  0.0236, -0.1922, -0.1472, -0.1606,  0.1187, -0.1062,  0.1669,\n",
            "         -0.1994,  0.0375,  0.1224,  0.1114,  0.0215, -0.1534, -0.2347,  0.2139],\n",
            "        [ 0.0215,  0.2202, -0.0103,  0.1490,  0.2247,  0.1188, -0.1763, -0.1927,\n",
            "         -0.0271, -0.1145, -0.0214,  0.1550,  0.0660, -0.1773,  0.1165,  0.1476],\n",
            "        [ 0.1826, -0.1476,  0.2030, -0.2104,  0.1691, -0.1083,  0.2320,  0.1494,\n",
            "          0.1730,  0.2145,  0.2462,  0.0389,  0.1488, -0.1784, -0.1780, -0.0566],\n",
            "        [ 0.0785, -0.2071,  0.1195,  0.1368, -0.0809,  0.1259,  0.0587,  0.1959,\n",
            "         -0.0179, -0.0849,  0.1513, -0.0772, -0.0920, -0.1968, -0.0854,  0.2206],\n",
            "        [ 0.0746, -0.1414,  0.1533,  0.0859,  0.0497, -0.0510,  0.0755, -0.0770,\n",
            "          0.0866,  0.2134,  0.0503,  0.1919, -0.0267, -0.1735,  0.0889,  0.0454],\n",
            "        [ 0.0184,  0.0832, -0.0893, -0.1844,  0.1191, -0.1582, -0.0014, -0.2143,\n",
            "          0.2418, -0.2106,  0.0665, -0.1384, -0.0114,  0.1401, -0.1730, -0.0621],\n",
            "        [-0.0935,  0.1850, -0.0786, -0.1553,  0.1784,  0.0290,  0.1616, -0.1227,\n",
            "          0.0961,  0.1678,  0.0902,  0.1985,  0.1283,  0.1642, -0.1830, -0.2470],\n",
            "        [ 0.2311, -0.2287, -0.2412,  0.1810,  0.2271,  0.1333,  0.1326,  0.1668,\n",
            "         -0.2381,  0.0968,  0.0325, -0.1525,  0.1233, -0.1782,  0.1233,  0.1462],\n",
            "        [-0.0686, -0.1679,  0.1713, -0.2237, -0.0540, -0.0061,  0.0980,  0.2286,\n",
            "          0.1767,  0.1057, -0.2458, -0.1054,  0.1671, -0.0188,  0.0094,  0.0724],\n",
            "        [ 0.0622, -0.0842, -0.0143, -0.2101, -0.0141, -0.0554, -0.0364, -0.1869,\n",
            "         -0.0921,  0.0718,  0.2222, -0.2474, -0.0978, -0.2116,  0.0874, -0.1075],\n",
            "        [ 0.1427,  0.2148, -0.1802, -0.1962,  0.0921, -0.1212, -0.0433,  0.0353,\n",
            "          0.1462, -0.1015,  0.0184, -0.2496, -0.1460,  0.0724,  0.1442,  0.1622],\n",
            "        [ 0.1997, -0.0648, -0.1810, -0.2475, -0.0661,  0.0890, -0.2290,  0.0727,\n",
            "         -0.1961, -0.0604, -0.0930,  0.0043, -0.1663,  0.1203,  0.0198,  0.0234]],\n",
            "       requires_grad=True)\n",
            "Output after forward pass: tensor([[ 1.9024e-01,  2.0925e-01, -2.9127e-02,  3.7773e-01, -5.2736e-01,\n",
            "         -6.3258e-01, -9.6765e-02,  1.8906e-01,  5.1569e-01,  6.0056e-01,\n",
            "          3.8097e-01,  4.0122e-01, -1.2673e-01,  2.8289e-01,  4.6002e-01,\n",
            "          4.3547e-02, -2.9769e-01, -1.3603e-01,  1.1266e-02,  1.1473e-01,\n",
            "         -3.6262e-01,  5.3166e-01,  5.5982e-01,  1.2003e-01,  7.1291e-01,\n",
            "          2.6417e-01,  5.8319e-01, -1.8685e-01,  7.5200e-02, -2.1492e-01,\n",
            "         -6.9538e-01, -1.5201e+00],\n",
            "        [ 2.0910e-01, -1.0411e-01, -2.7647e-01, -3.1305e-01,  6.8314e-01,\n",
            "         -6.1447e-01,  1.6435e-01, -2.0588e-01,  8.1064e-01,  4.1270e-01,\n",
            "          4.1350e-01, -1.6416e-01, -4.3495e-01,  1.1222e-01,  4.9724e-01,\n",
            "          2.4394e-01, -1.7673e-01, -2.8261e-01,  3.0788e-02,  8.3939e-01,\n",
            "          1.2365e+00,  1.4401e-01,  2.6207e-01,  8.2305e-01,  5.2200e-01,\n",
            "         -4.2836e-01, -4.0658e-01,  1.0162e+00,  4.5628e-01,  4.1886e-02,\n",
            "         -6.1425e-02,  3.8275e-01],\n",
            "        [-4.1765e-01, -3.9122e-01, -1.2117e-01,  4.4604e-01,  1.1256e-01,\n",
            "          4.7684e-01,  3.4389e-01, -7.4738e-01, -3.5594e-01,  5.8982e-01,\n",
            "         -6.0792e-01, -1.1457e-03, -5.3838e-02,  1.3467e+00, -1.2175e+00,\n",
            "         -9.9356e-01, -7.0000e-01, -7.2960e-01, -1.0186e+00,  1.0422e+00,\n",
            "          3.2977e-01,  4.0288e-01, -1.2081e+00,  1.5485e-01, -6.3994e-01,\n",
            "          2.2189e-01, -8.6347e-01,  1.2487e+00,  4.0017e-01, -3.1617e-01,\n",
            "          3.0849e-01,  2.0091e-01],\n",
            "        [ 5.7803e-01, -1.6379e-01,  1.1602e+00, -3.8024e-01,  5.5440e-01,\n",
            "         -3.0010e-01, -4.9087e-01,  2.9728e-01, -4.9179e-01,  7.7226e-01,\n",
            "          6.1241e-01, -4.7586e-01, -4.7288e-01, -8.9808e-01, -1.9581e-01,\n",
            "          8.8843e-01,  5.5661e-02,  1.3236e+00,  1.1493e+00,  7.5390e-01,\n",
            "         -8.0263e-02,  3.4773e-01, -4.9944e-02,  6.8110e-02,  9.6790e-01,\n",
            "         -1.3713e-02,  6.8592e-01,  4.6431e-01, -4.9351e-01, -8.2859e-01,\n",
            "         -7.4450e-01, -7.1606e-02]], grad_fn=<AddmmBackward0>)\n",
            "Shape: torch.Size([4, 32])\n",
            "Model: \n",
            "Sequential(\n",
            "  (0): Linear(in_features=784, out_features=32, bias=True)\n",
            "  (1): Linear(in_features=32, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# We now build our first neural network layers and combine them into one model\n",
        "\n",
        "# First lets define an example Linear layer.\n",
        "# Initialize a Linear layer from PyTorch of dimension (in_features=16, out_features=32).\n",
        "\n",
        "# Code here\n",
        "layer = nn.Linear(in_features=16, out_features=32)\n",
        "\n",
        "\n",
        "# Print the layer attributes and print the weight of the Linear layer\n",
        "# Forward a fitting random initialized 2d tensor through the layer and print the result\n",
        "# What is the shape of the passed (forwarded) random tensor?\n",
        "\n",
        "# Code here\n",
        "print(f\"Layer: {layer}\")\n",
        "print(f\"Weight of Layer: {layer.weight}\")\n",
        "\n",
        "# random 2d tensor\n",
        "input = torch.randn(4, 16)\n",
        "\n",
        "# forward pass\n",
        "output = layer(input)\n",
        "\n",
        "print(f\"Output after forward pass: {output}\")\n",
        "print(f\"Shape: {output.shape}\")\n",
        "\n",
        "\n",
        "# Why does it work to just call an initialized layer by initialized_layer(input)?\n",
        "# Check the source code for the Linear Layer and its parent 'Module' class here: https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html#Linear\n",
        "# Explain in your own words why the forward function of the 'Linear' class is automatically called when passing an input through the layer, i. e. initialized_layer(input)\n",
        "# Hint: Check out the class inheritance!\n",
        "\n",
        "# Your explanation here\n",
        "# When we are initializing a layer through nn.Linear and then calling it with an input, we are invoking a __call__ method\n",
        "# Forward function is automatically called because of the input we pass to the Module (in this case it is Linear)\n",
        "\n",
        "\n",
        "# Build a sequential model with some linear layers stacked after each other. The number of layers is your choice, but be careful because it could cost a lot of time\n",
        "# to pass data through the sequential model afterwards. Start e. g. with three linear layers :)\n",
        "# You are not restricted to linear layers. Experiment a little bit here!\n",
        "\n",
        "# Code here\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(784, 32),  # input layer (do not change the in_features size of this layer - we need it later)\n",
        "    # your layers\n",
        "    nn.Linear(32, 10)  # you can change the in_features of this layer but let the out_features at size 10 here - we need it layer\n",
        ")\n",
        "print(f\"Model: \\n{model}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIVHKlNLUeM0"
      },
      "source": [
        "## PyTorch Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "-FL_iWk2UeM0",
        "outputId": "4288029a-42a5-41ea-fd11-ea8efac467f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data: \n",
            "tensor([[-1.3427, -1.1554, -1.6629,  ..., -1.7823, -0.6915,  0.7108],\n",
            "        [-0.3047,  0.0372, -1.0374,  ..., -0.5332, -0.8153,  0.5508],\n",
            "        [ 0.6148,  0.8266, -2.6018,  ...,  2.0162,  0.2203,  0.2922],\n",
            "        [-0.4416,  0.1026, -0.1778,  ..., -2.8853, -0.6033,  1.0088],\n",
            "        [-0.9524,  1.1397, -0.9669,  ..., -1.0141, -0.4408,  2.3167]])\n",
            "Output: \n",
            "tensor([[-0.4855,  0.2118,  0.3342,  0.1344,  1.0247,  0.5006,  0.2500,  0.5091,\n",
            "         -0.2670, -0.2367],\n",
            "        [-0.2360,  1.0135,  0.0352,  0.6880,  0.3433,  0.4334,  0.2968,  1.1326,\n",
            "         -0.0584,  0.0186],\n",
            "        [ 0.1423,  0.0838, -0.3979,  0.1080,  0.4041,  0.4671,  0.1904, -0.0684,\n",
            "          0.2462, -0.1559],\n",
            "        [-0.0816,  0.0413,  0.0317, -0.2672, -0.3949,  0.2872,  0.4651,  0.4121,\n",
            "         -0.0825, -0.1804],\n",
            "        [-0.0297, -0.0228, -0.2338, -0.0280, -0.5789,  0.0589,  0.1739, -0.6713,\n",
            "         -0.0811,  0.4149]], grad_fn=<AddmmBackward0>)\n",
            "Output Shape: torch.Size([5, 10])\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-88de0589-b392-4106-81cc-c32bf90d957f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-88de0589-b392-4106-81cc-c32bf90d957f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving mnist_9.jpg to mnist_9.jpg\n",
            "torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAM4UlEQVR4nO3cO4jd5drG4Xdm1hwzGY1HxCaQuCuLEGzioRa1CYgBO0ERTGE6wcJKEDsDFtpHFBFMkSJIChW0EIIyImITbD0UiZM5n9bubj7YHx/reb/MJJl9XfXcrH+cNfObVfiMDYfDYQOA1tr47X4AAO4cogBAiAIAIQoAhCgAEKIAQIgCACEKAMRg1C8cGxvby+fgFllYWChvlpaW9uBJbp3p6enypuf/ydzc3Cxv9tPU1FR5MxiM/CMeq6ur5c2hQ4fKm5WVlfKmtdYmJibKm52dna7XOmhG+bnwSQGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgxoYjXg5zEI//aXJysrzpfQ/dyYfqeo7UbW9vd73W7u5u124/9Bze69m01tr6+nrXDgfxACgSBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACAcxDtgZmdny5ueA2Mjvm1uiYmJifKm5/26X/+m6enprt3q6mp5Mz8/X94sLy+XN3e6hYWF8mZpaWkPnuT2chAPgBJRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhXUunSc7l0cnKy67V6rpdubm7uy+scRD3f28FgUN5sbGyUN/z/uJIKQIkoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCAFG/YsUd7d577y1vVldXy5ueg3M7OzvlzX4aH6//jdRzRG96erq8aa3vv1/PIcue7+3U1FR50+vIkSPlzdraWnmzvr5e3hwEPikAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAxNhwxItePYe1OLhmZmbKm57jca21tr29vS+v1fMe79n0/HvudAsLC+XN0tLSHjwJ/5dRfi58UgAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFACIwe1+AG6/6enp8qbn4NzGxkZ50+v48ePlzfPPP1/enDp1qrxZX18vb1pr7Zlnnilvjh07Vt7cuHGjvPniiy/Km4sXL5Y3rbV2+fLl8mZ2dra8WVtbK28OAp8UAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAGJsOOJls7Gxsb1+Fm6BmZmZ8qb3QFvVE0880bV79dVXy5sXX3yxvHnwwQfLm57DgL0/S8vLy+XN+Hj97765ubnypue/w9LSUnnTWmuPPvpoebOystL1WgfNKN8nnxQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQAiMHtfgBurf26ePrKK6+UNx9++GHXa/Vcfh0M6m/tP/74o7z54Ycfypsvv/yyvGmttV9//bW8WVxcLG+2trbKm6effrq8eeSRR8qb1lw83Ws+KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgDE2HA4HI70hWNje/0s3ALj4/XOnzx5srz59ttvy5u5ubnyprXWtre3y5vz58+XN5999ll58+OPP5Y3vaampsqbnvfDzs5OedNzRK9Xz4HEjY2N8mbEX413lVH+TT4pABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAISDeAfMv/71r/Lmt99+K29WVlbKm/X19fKmtdbOnDlT3nz99dflzWAwKG96Ds71vE5rra2urnbtqo4ePVrenD17trzpeQ+11tp7771X3uznwb47mYN4AJSIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABB9l7m4Y62trZU3PcfC5ufny5tvvvmmvGmttcXFxa5d1fb29r68zubmZteu5/jeyZMny5vLly+XNw888EB5c+3atfKmtdbefffdrh2j8UkBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBgbDofDkb5wbGyvn4Xb5OLFi+XN6dOny5vd3d3yprXW3n///fJmeXm5vPn555/Lm7///ru8OXToUHnTWmsvv/xyeXPq1Kny5vHHHy9vehw/frxr13NddTCoH4Ter6u5+2mUX/c+KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEg3gHzPz8fHlz7Nix8ubKlSvlzcTERHnTWmv33XdfebO1tVXeXL9+vbx56KGHypvew4Dj4/vzN9zS0lJ588ILL5Q33333XXnTWmuHDx8ub27evNn1WgeNg3gAlIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEA7i0SYnJ8ubhx9+uLw5f/58edNaa4899lh5MxgMypupqany5ujRo+VNz7O11tr29va+vNZrr71W3ly4cKG8mZ2dLW9aa+2ff/4pb3p+f434q/Gu4iAeACWiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAISDeAfM3NxcebO6ulrezM/PlzdbW1vlTWutbWxsdO2q7r///vLmo48+Km9Onz5d3rTWd7hwcXGxvDlx4kR502N8vO9v0t3d3Vv8JP89HMQDoEQUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBjc7gfg1lpfXy9veg6TLS8vlzfT09PlTWutDQb1t+m5c+fKmzfeeKO8OXbsWHmzny5dulTe9Bze6zl22PM6rfUdSOw56DnirdADxycFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAGJsOOIpwJ4rgxxcPRdPe65bttbak08+Wd58//335c3S0lJ5s7CwUN78/vvv5U1rrX388cflzQcffFDe9Fw8nZmZKW96f6esra2VN7Ozs/vyOne6UX7d+6QAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEIPb/QDcWj1Hxubm5sqbnqNpU1NT5U1rrT333HPlzfXr18ubI0eOlDd//vlneXPmzJnyprXWFhcXy5ue79M999xT3ty8ebO82d3dLW96HcTjdnvFJwUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAcBDvgBkOh+XNysrKHjzJf3rnnXe6dm+99VZ5s76+Xt4sLy+XN+fOnStvrl69Wt706jlC2HM8bj+P283Pz5c3Pd/b/1Y+KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEg3gHzOTkZHmztbVV3vQcgjt79mx509r+HXV7++23y5vPP/+8vDl8+HB501prN2/eLG82Nze7XutO1nPAcWJiorzZ2dkpbw4CnxQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAYmw4HA5H+sKxsb1+Fm6Tp556qry5cuVKeTM7O1vetNbajRs3yps333yzvLlw4UJ5MxjUb0pub2+XN631/Qz2/DfvOR7X82/qOVrYq+eo4kE8JjjKr3ufFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFACI+olHDpxnn322vOm5Orm8vFzetNbaV199Vd58+umnXa+1H3ovDvfsVldXu15rP/Reze2xnxdZ73Y+KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgDE2HA4HI70hZ1HvNhfL730UnnzySeflDc7OzvlzV9//VXetNba0aNHy5vx8frfOzMzM+VNz8G5nmOCrbW2ublZ3kxMTJQ3k5OT5c36+np502t6erq82djY2IMnufuM8uveJwUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAcBDvgPnpp5/KmxMnTuzL67z++uvlTWutXb16tbzpeb+O+KMAdy0H8QAoEQUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgBrf7Abi1rl27Vt4sLy+XN5cuXSpvfvnll/LmTjc5OVnebG1t7cGTwK3hkwIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAMTYcDocjfeHY2F4/C7fJzMxMebO+vr4HT/K/Gx+v/+0yNzdX3vRci4W7ySi/7n1SACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAgH8Q6YwWBQ3vQcnOsx4lvtP/S89zY3N7teCw4yB/EAKBEFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEa+ntZ7zAyAu4dPCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAMS/AZtulvBrmh4qAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# We initialized our model in the previous section\n",
        "# Lets now also use the model to pass data through it\n",
        "\n",
        "# Use the following tensor and pass it through your model from above\n",
        "# You have to 'reformat' the tensor first\n",
        "data = torch.randn(size=(5, 1, 28, 28))\n",
        "\n",
        "# Code here\n",
        "data = data.view(data.size(0), -1)\n",
        "print(f\"Data: \\n{data}\")\n",
        "\n",
        "output = model(data)\n",
        "print(f\"Output: \\n{output}\")\n",
        "print(f\"Output Shape: {output.shape}\")\n",
        "\n",
        "# read the image 'mnist_9.jpg' from the downloaded folder with the 'torchvision' python package and pass it through the network\n",
        "# How does the tensor of the image looks like? Which information is in the different dimensions?\n",
        "\n",
        "# Code here\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "image_path = \"mnist_9.jpg\"\n",
        "img = Image.open(image_path)\n",
        "transform = transforms.ToTensor()\n",
        "image = transform(img)\n",
        "\n",
        "print(image.shape)\n",
        "\n",
        "# visualize the image from above with matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Code here\n",
        "image = image.reshape(28,28,1)\n",
        "plt.imshow(image, cmap=\"gray\")\n",
        "plt.axis(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTdBNqtSUeM0"
      },
      "source": [
        "## PyTorch Neural Network Example Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn78CHi_UeM0",
        "outputId": "2fb081c4-8286-4491-a6b2-ddf890c1a016"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 11.9MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 350kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.76MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.56MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training iteration 1: 100%|██████████| 15000/15000 [01:16<00:00, 196.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 finished with loss: 0.430 and accuracy 0.871\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training iteration 2: 100%|██████████| 15000/15000 [01:14<00:00, 200.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 finished with loss: 0.363 and accuracy 0.896\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training iteration 3: 100%|██████████| 15000/15000 [01:15<00:00, 198.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 finished with loss: 0.347 and accuracy 0.900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training iteration 4: 100%|██████████| 15000/15000 [01:15<00:00, 197.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 finished with loss: 0.338 and accuracy 0.902\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training iteration 5: 100%|██████████| 15000/15000 [01:17<00:00, 193.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 finished with loss: 0.330 and accuracy 0.905\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training iteration 6: 100%|██████████| 15000/15000 [01:13<00:00, 203.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 finished with loss: 0.326 and accuracy 0.907\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training iteration 7: 100%|██████████| 15000/15000 [01:18<00:00, 190.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 finished with loss: 0.321 and accuracy 0.908\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training iteration 8: 100%|██████████| 15000/15000 [01:16<00:00, 196.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 finished with loss: 0.317 and accuracy 0.911\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training iteration 9: 100%|██████████| 15000/15000 [01:16<00:00, 197.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 finished with loss: 0.315 and accuracy 0.910\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training iteration 10: 100%|██████████| 15000/15000 [01:14<00:00, 200.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 finished with loss: 0.311 and accuracy 0.912\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# This is only the application of your defined model\n",
        "# You can use the following method to train your model and check its accuracy. You can also use parts of the code below for the following practicals.\n",
        "# Just execute this box and it uses the predefined model from the previous task to run a training procedure. The variable name of the model must be 'model' (or change it accordingly).\n",
        "# ATTENTION: No worries if you don't understand the implementation. This is just for showing you how your defined model performs in terms of accuracy.\n",
        "# We will discuss everything in this code in future practicals.\n",
        "\n",
        "# Refine your model multiple times and see how the different models perform in terms of accuracy.\n",
        "\n",
        "# We use the MNIST dataset to set the model\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import tqdm\n",
        "\n",
        "def load_mnist_data(root_path='./data', batch_size=4):\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5), (0.5))]\n",
        "    )\n",
        "\n",
        "    trainset = torchvision.datasets.MNIST(root=root_path, train=True, download=True, transform=transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.MNIST(root=root_path, train=False, download=True, transform=transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return trainloader  , testloader\n",
        "\n",
        "\n",
        "def train_model(model, batch_size: int = 4, epochs: int = 10):\n",
        "    # we only consider the mnist train data for this example\n",
        "    train_loader, _ = load_mnist_data(root_path='./data', batch_size=batch_size)\n",
        "\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    model = model.to(device=device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        running_accuracy = []\n",
        "        for imgs, targets in tqdm.tqdm(train_loader, desc=f'Training iteration {epoch + 1}'):\n",
        "            imgs, targets = imgs.to(device=device), targets.to(device=device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = model(imgs.reshape(imgs.shape[0], -1))\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate the Accuracy (how many of all samples are correctly classified?)\n",
        "            max_outputs = torch.max(outputs, dim=1).indices\n",
        "            accuracy = (max_outputs.detach() == targets.detach()).to(dtype=torch.float32).mean()\n",
        "            running_accuracy.append(accuracy)\n",
        "\n",
        "        print(f'Epoch {epoch + 1} finished with loss: {running_loss / len(train_loader):.3f} and accuracy {torch.tensor(running_accuracy).mean():.3f}')\n",
        "\n",
        "\n",
        "# Run the model training with the name of your model variable, in this case 'model'\n",
        "train_model(model=model, batch_size=4, epochs=10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "25301cabe4c6f833fd20f15b1b22933971919908771eb627a83fe325b4fb6671"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
