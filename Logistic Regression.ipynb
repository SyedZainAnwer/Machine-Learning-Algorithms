{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "865d7567",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261f1031",
   "metadata": {},
   "source": [
    "Implement a logistic (binary) regression model and use your stochastic gradient descent approach from the last practicals to optimize the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "910c0677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can import your code from a different notebook like follows (change it to your path)\n",
    "\n",
    "# %run ..\\5_SGD_Linear_Regression\\SGD_Solution.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc5873",
   "metadata": {},
   "source": [
    "## Data for the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5d124ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "743256fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting data for training and test are in (X_train, y_train) and (X_test, y_test), respectively.\n",
    "\n",
    "# Lets assume we have some data points that define the data as 'fraud' or 'no fraud', e. g. in bank account scenarios.\n",
    "# We therefore have three values given ('number of withdrawals per week', 'avg_withdrawal_amount', 'number_of_different_addressees').\n",
    "\n",
    "data_amount = 15\n",
    "frauds_amount = data_amount // 2\n",
    "no_frauds_amount = data_amount - frauds_amount\n",
    "\n",
    "### Generate (fake) 'fraud' data\n",
    "# number of withdrawals per week (fraud) - between 0 and 0.5\n",
    "X_1_f = np.random.rand(frauds_amount) / 2\n",
    "# average withdrawal amount 'fraud' - between 0.5 and 1\n",
    "X_2_f = (np.random.rand(frauds_amount) + 1) / 2\n",
    "# number of different addressees 'fraud' - between 0 and 0.5\n",
    "X_3_f = np.random.rand(frauds_amount) / 2\n",
    "\n",
    "X_f = np.stack([X_1_f, X_2_f, X_3_f], axis=1)\n",
    "\n",
    "# Labels of 'fraud' (1)\n",
    "y_f = np.ones(frauds_amount)\n",
    "\n",
    "### Generate (fake) 'no fraud' data - between 0.5 and 1\n",
    "X_1_nf = (np.random.rand(no_frauds_amount) + 1) / 2\n",
    "# average withdrawal amount 'no fraud' - between 0 and 0.5\n",
    "X_2_nf = np.random.rand(no_frauds_amount) / 2\n",
    "# number of different addressees 'no fraud' - between 0.5 and 1\n",
    "X_3_nf = (np.random.rand(no_frauds_amount) + 1) / 2\n",
    "\n",
    "X_nf = np.stack([X_1_nf, X_2_nf, X_3_nf], axis=1)\n",
    "\n",
    "# Labels of 'no fraud' (0)\n",
    "y_nf = np.zeros(no_frauds_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d8549",
   "metadata": {},
   "source": [
    "## Shuffle fraud and no fraud data to create a mixed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "753d811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them (concatenate)\n",
    "X = np.concatenate((X_f, X_nf))\n",
    "y = np.concatenate((y_f, y_nf))\n",
    "\n",
    "# now randomly shuffle them\n",
    "shuffled_indices = np.random.choice(y.shape[0], size=y.shape[0], replace=False)\n",
    "X = X[shuffled_indices]\n",
    "y = y[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10d5ca7",
   "metadata": {},
   "source": [
    "## Split into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f4d5de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = int(data_amount * 0.75)\n",
    "\n",
    "# We train with the following data\n",
    "X_train = X[:train_len]\n",
    "y_train = y[:train_len]\n",
    "\n",
    "# We test / evaluate with the following data\n",
    "X_test = X[train_len:]\n",
    "y_test = y[train_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2880fc3",
   "metadata": {},
   "source": [
    "## Initialize the weights of your logistic regression model (see SGD exercise on how to do this with numpy - dont forget to initialize the bias nodes as well!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "155071b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_weights = np.random.rand(X.shape[1])\n",
    "final_weights = final_weights / np.sum(final_weights)\n",
    "\n",
    "final_bias = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff9268",
   "metadata": {},
   "source": [
    "## Define the loss function (derivative) for your logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fe131ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = lambda y_true, y_pred: -np.mean(np.sum(y_true * np.log(y_pred), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e102bd",
   "metadata": {},
   "source": [
    "## Define the activation function for your logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a13e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint:  How do you scale your output for logistic regression? What is the range?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c133b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use your Stochastic Gradient Descent approach from the previous exercise and optimize your weights.\n",
    "# If your SGD implementation cannot do this, adjust the function implementation until it is able to do it :)\n",
    "\n",
    "learning_rate = 0.005\n",
    "iterations = 1000\n",
    "\n",
    "def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "class Logistic_Regression:\n",
    "    def __init__(self, learning_rate=0.005, iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        number_of_samples, number_of_features = X_train.shape\n",
    "        self.weights = np.zeros(number_of_features)\n",
    "        self.bias = 0\n",
    "        for i in range(self.iterations):\n",
    "                linear_pred = np.dot(X_train, self.weights) + self.bias\n",
    "                y_pred = sigmoid(linear_pred)\n",
    "                dw = (1 / number_of_samples) * np.dot(X_train.T, (y_pred - y_train))\n",
    "                db = (1 / number_of_samples) * np.sum(y_pred - y_train)\n",
    "\n",
    "                self.weights -= self.learning_rate * dw\n",
    "                self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        linear_pred = np.dot(X_test, self.weights) + self.bias\n",
    "        y_pred = sigmoid(linear_pred)\n",
    "        class_pred = [0 if y<=0.5 else 1 for y in y_pred]\n",
    "        return class_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8927dd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1]\n",
      "[0. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Results\n",
    "model = Logistic_Regression()\n",
    "model.fit(X_train,y_train)\n",
    "result = model.predict(X_test)\n",
    "\n",
    "print(result)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f144a8",
   "metadata": {},
   "source": [
    "## Predict the values for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb00461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
